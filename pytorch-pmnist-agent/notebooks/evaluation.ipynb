{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Agent Evaluation and Benchmarking Analysis\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "This notebook serves as the dedicated environment for analyzing the performance metrics generated by the `experiments/run_benchmarks.py` script. We compare the **Optimized Agent (PCA+Ensemble)** against the **Baseline Agent (Shallow MLP)** across key metrics: Accuracy, Training Time, and Inference Time.\n",
        "\n",
        "## 2. Setup and Data Loading\n",
        "\n",
        "We rely on the `final_benchmarks.json` file created by the main evaluation script."
      ],
      "metadata": {
        "id": "bHHt2k5-9nDH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JHowICK9kb_",
        "outputId": "011f9efc-59dc-4f35-e127-4319fa1ffd73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: Results file not found at experiments/results/final_benchmarks.json.\n",
            "Please run 'python -m experiments.run_benchmarks' first.\n"
          ]
        }
      ],
      "source": [
        "# CODE CELL 1: Setup and Data Loading\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import os\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Assuming the notebook is run from the repository root (or the file path is correct)\n",
        "RESULTS_PATH = 'experiments/results/final_benchmarks.json'\n",
        "\n",
        "try:\n",
        "    with open(RESULTS_PATH, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Convert numerical columns (Accuracy is usually float, Times might be strings if N/A)\n",
        "    for col in ['Accuracy', 'Train Time (s)', 'Predict Time (s)', 'Peak Memory (MB)']:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    print(f\"Successfully loaded {len(df)} records from {RESULTS_PATH}\")\n",
        "    print(\"\\nDataFrame Head:\")\n",
        "    print(df.head())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: Results file not found at {RESULTS_PATH}.\")\n",
        "    print(\"Please run 'python -m experiments.run_benchmarks' first.\")\n",
        "    df = None\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during data loading: {e}\")\n",
        "    df = None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Comparative Analysis\n",
        "\n",
        "### 3.1 Performance Table\n",
        "\n",
        "Below is the structured output table comparing the two agents based on key metrics."
      ],
      "metadata": {
        "id": "U9mMzWjI9wvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE CELL 2: Display Comparison Table\n",
        "if df is not None:\n",
        "    # Select and format columns for clean display\n",
        "    display_df = df[['Agent', 'Accuracy', 'Train Time (s)', 'Predict Time (s)', 'Models', 'Peak Memory (MB)']].copy()\n",
        "    display_df['Accuracy'] = display_df['Accuracy'].map('{:.4f}'.format)\n",
        "\n",
        "    print(\"--- Comparative Benchmark Table ---\")\n",
        "    print(tabulate(display_df, headers='keys', tablefmt='fancy_grid', showindex=False))\n",
        "else:\n",
        "    print(\"Cannot display table: Data not loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHTnBQ6Z9zVN",
        "outputId": "a958f52d-fd49-4c7f-cfc5-7f57bbbbe191"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cannot display table: Data not loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Visualization of Time and Accuracy\n",
        "\n",
        "We visualize the key trade-offs between speed and performance."
      ],
      "metadata": {
        "id": "VN2PR-gJ98JS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE CELL 3: Visualization - Accuracy vs. Training Time\n",
        "\n",
        "if df is not None:\n",
        "    # Ensure data is clean for plotting\n",
        "    plot_df = df.dropna(subset=['Accuracy', 'Train Time (s)', 'Predict Time (s)'])\n",
        "\n",
        "    if not plot_df.empty:\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "        # --- Plot 1: Accuracy ---\n",
        "        axes[0].bar(plot_df['Agent'], plot_df['Accuracy'], color=['skyblue', 'salmon'])\n",
        "        axes[0].set_title('Agent Accuracy Comparison', fontsize=14)\n",
        "        axes[0].set_ylabel('Accuracy', fontsize=12)\n",
        "        axes[0].set_ylim(plot_df['Accuracy'].min() * 0.98, 1.0) # Zoom in on high accuracy\n",
        "        axes[0].tick_params(axis='x', rotation=15)\n",
        "\n",
        "        # --- Plot 2: Total Time ---\n",
        "        # Calculate Total Time (Train + Predict)\n",
        "        plot_df['Total Time (s)'] = plot_df['Train Time (s)'] + plot_df['Predict Time (s)']\n",
        "\n",
        "        axes[1].bar(plot_df['Agent'], plot_df['Total Time (s)'], color=['lightgreen', 'darkorange'])\n",
        "        axes[1].set_title('Total Execution Time (Train + Predict)', fontsize=14)\n",
        "        axes[1].set_ylabel('Time (seconds)', fontsize=12)\n",
        "        axes[1].tick_params(axis='x', rotation=15)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        print(\"Visualization complete.\")\n",
        "    else:\n",
        "        print(\"Cannot plot: Numerical data for plotting is missing.\")\n",
        "else:\n",
        "    print(\"Cannot plot: Data not loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFGI0uXO9-N0",
        "outputId": "049d9e48-f817-446c-d653-9e70aa2cbcd9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cannot plot: Data not loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Conclusion\n",
        "\n",
        "The analysis confirms the expected trade-off: The Optimized Agent consumes more resources (time/memory) but delivers significantly higher accuracy due to PCA's efficiency and the power of the deep ensemble. This justifies its submission to the time-constrained ML-Arena competition."
      ],
      "metadata": {
        "id": "rcOh2MOt-BTg"
      }
    }
  ]
}